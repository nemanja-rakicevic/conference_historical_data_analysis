{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeurIPS historical conference data analysis\n",
    "\n",
    "I was reading an old NIPS paper form 2005, and was unconsciously surprised that there's no appendix, so I suddenly got an urge to check when did appendices start becoming a 'trend'... and as I suspected it's almost a step function:\n",
    "\n",
    "[PLOT]\n",
    "\n",
    "Since I already had the data, I did some other statistical analysis of the conference data, check it out: [BLOG]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the organisers of NeurIPS all the accepted paper data is neatly available on https://papers.nips.cc/\n",
    "\n",
    "Unfortunately, there would be also many insights from rejected papers, but these are not available. The total number of submissions and acceptance rate is given in the press kit, but it's in PDF which is difficult to parse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the information was available for each paper from the main website, and I used some additional tools to get other info. the following is available:\n",
    "- title\n",
    "- year\n",
    "- author names and [some] affiliation\n",
    "- suplementary material indicator (yes/no)\n",
    "- [some] abstract\n",
    "- [some] paper text\n",
    "- [some] reviews\n",
    "- [some] number of citations (scraped from scholar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gender_guesser.detector as gender\n",
    "detector = gender.Detector()\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "world_population = {}\n",
    "neurips_locations = {}\n",
    " \n",
    "\n",
    "\n",
    "def get_institution_country(institution_name):\n",
    "    search = \"https://en.wikipedia.org/wiki/{}\".format(institution_name)\n",
    "    try:\n",
    "        html_text = requests.get(search).text\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        if hasattr(soup.find(\"div\", attrs={'class': ['country-name']}).contents[-1], 'text'):\n",
    "            return soup.find(\"div\", attrs={'class': ['country-name']}).contents[-1].text\n",
    "        else:\n",
    "            return str(soup.find(\"div\", attrs={'class': ['country-name']}).contents[-1])\n",
    "    except:\n",
    "        try:\n",
    "            if hasattr(soup.find(\"span\", attrs={'class': ['country-name']}).contents[-1], 'text'):\n",
    "                return soup.find(\"span\", attrs={'class': ['country-name']}).contents[-1].text\n",
    "            else:\n",
    "                return str(soup.find(\"span\", attrs={'class': ['country-name']}).contents[-1])\n",
    "        except:\n",
    "            try:\n",
    "                if hasattr(soup.find(\"span\", attrs={'class': ['locality']}).contents[-1], 'text'):\n",
    "                    return soup.find(\"span\", attrs={'class': ['locality']}).contents[-1].text\n",
    "                else:\n",
    "                    return str(soup.find(\"span\", attrs={'class': ['locality']}).contents[-1])\n",
    "            except:\n",
    "                try:\n",
    "                    return re.search('class=\"country-name\">(.*?)<', html_text).group(1)\n",
    "                except:\n",
    "                    try:\n",
    "                        return re.search('class=\"country-name\"><a href=\"/wiki/(.*?)\"', html_text).group(1)\n",
    "                    except:\n",
    "                        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'neurips_conf_data.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1e1e2a060310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neurips_conf_data.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mconf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mavg_citation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'neurips_conf_data.pkl'"
     ]
    }
   ],
   "source": [
    "with open('data/neurips_conf_data.pkl', 'rb') as handle:\n",
    "    conf_data = pickle.load(handle)\n",
    "    \n",
    "\n",
    "avg_citation = []\n",
    "avg_supplementary = []\n",
    "total_supplementary = []\n",
    "avg_abstract_len = []\n",
    "avg_num_authors = []\n",
    "\n",
    "total_authors = {}\n",
    "total_institutions = {}\n",
    "\n",
    "phd_authors = {}\n",
    "\n",
    "year_stats = {}\n",
    "year_stats['n_accepted'] = []\n",
    "\n",
    "year_stats['unique_authors'] = []\n",
    "year_stats['unique_authors_info'] = []\n",
    "year_stats['unique_institutions'] = []\n",
    "year_stats['unique_institutions_info'] = []\n",
    "\n",
    "year_stats['avg_authors_per_paper'] = []\n",
    "year_stats['avg_institutions_per_paper'] = []\n",
    "\n",
    "year_stats['avg_papers_per_author'] = []\n",
    "year_stats['avg_papers_per_institution'] = []\n",
    "\n",
    "year_stats['review_avg_num'] = []\n",
    "year_stats['review_avg_confidence'] = []\n",
    "year_stats['review_avg_length'] = []\n",
    "\n",
    "\n",
    "year_stats['len_title'] = []\n",
    "year_stats['abbrev_title'] = []\n",
    "year_stats['title_ayn'] = []\n",
    "\n",
    "for kk in sorted(conf_data.keys()):\n",
    "    # Total num papers\n",
    "    vals = conf_data[kk]\n",
    "    year_stats['n_accepted'].append(len(vals))\n",
    "    # Average num citations\n",
    "    list_n_citations = [vv['citations'] for vv in vals if vv['citations'] is not None]\n",
    "#     avg_citation.append(list_n_citations)\n",
    "    avg_citation.append(np.mean(list_n_citations))\n",
    "    # Average num supplementary\n",
    "    list_n_supplementary = [vv['has_supplement'] for vv in vals]\n",
    "    avg_supplementary.append(np.mean(list_n_supplementary) * 100)\n",
    "    total_supplementary.append(np.sum(list_n_supplementary))\n",
    "#     # Average abstract length\n",
    "#     list_abstract_len = [len(vv['abstract']) for vv in vals if vv['abstract'] is not None and len(vv['abstract'])>0]\n",
    "#     avg_abstract_len.append(np.mean(list_abstract_len))\n",
    "\n",
    "\n",
    "    # Unique authors and institutions\n",
    "    author_papers = {}\n",
    "    institution_papers = {}\n",
    "    for paper in vals:\n",
    "        # yearly papers per author\n",
    "        for a_idx, a_name in enumerate(paper['authors']):\n",
    "            ahash = '{} {}'.format(a_name['given_name'], a_name['family_name'])  # ahash = '{}_{}'.format(a_name['given_name'].upper(), a_name['family_name'].upper())\n",
    "            if ahash in author_papers.keys():\n",
    "                author_papers[ahash]['n_papers'].append((a_idx + 1, len(paper['authors'])))\n",
    "            else:\n",
    "                author_papers[ahash] = {}\n",
    "                author_papers[ahash]['n_papers'] = [(a_idx + 1, len(paper['authors']))]\n",
    "                author_papers[ahash]['gender'] = detector.get_gender(ahash.split()[0])\n",
    "        # yearly papers per institution\n",
    "        for i_name in paper['institutions']:\n",
    "            if i_name is None or i_name == '':\n",
    "                continue\n",
    "            if i_name in institution_papers.keys():\n",
    "                institution_papers[i_name] += 1\n",
    "            else:\n",
    "                institution_papers[i_name] = 1\n",
    "    year_stats['unique_authors'].append(len(author_papers))\n",
    "    year_stats['unique_authors_info'].append(author_papers)\n",
    "    year_stats['unique_institutions'].append(len(institution_papers))\n",
    "    year_stats['unique_institutions_info'].append(institution_papers)\n",
    "    \n",
    "    year_stats['avg_papers_per_author'].append(np.mean([len(vv['n_papers']) for vv in author_papers.values()]))\n",
    "    year_stats['avg_papers_per_institution'].append(np.mean(list(institution_papers.values())))\n",
    "    \n",
    "    # Average authors and institutions per paper\n",
    "    list_num_authors = [len(vv['authors']) for vv in vals]\n",
    "    year_stats['avg_authors_per_paper'].append(np.mean(list_num_authors))\n",
    "    \n",
    "    list_all_authors = [vv['authors'] for vv in vals]\n",
    "    list_institutions = [len(set([aff['institution'] for aff in af if aff['institution'] is not None and aff['institution']!=''])) for af in list_all_authors]\n",
    "    year_stats['avg_institutions_per_paper'].append(np.mean(list_num_authors))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Update total authors and institutions\n",
    "    for t_author in author_papers.keys():\n",
    "        if t_author in total_authors.keys():\n",
    "            total_authors[t_author]['n_papers'].extend(author_papers[t_author]['n_papers'])\n",
    "        else:\n",
    "            total_authors[t_author] = {}\n",
    "            total_authors[t_author]['n_papers'] = author_papers[t_author]['n_papers'].copy()\n",
    "            total_authors[t_author]['gender'] = author_papers[t_author]['gender']\n",
    "            \n",
    "    for t_inst in institution_papers.keys():\n",
    "        if t_inst in total_institutions.keys():\n",
    "            total_institutions[t_inst] += institution_papers[t_inst]\n",
    "        else:\n",
    "            total_institutions[t_inst] = institution_papers[t_inst]\n",
    "            \n",
    "    # Update PhD productivity\n",
    "    for phd_auth in author_papers.keys():\n",
    "        if phd_auth in phd_authors.keys():\n",
    "            if int(kk) <= phd_authors[phd_auth]['start_date'] + 5:\n",
    "                phd_authors[phd_auth]['n_papers'].extend(author_papers[phd_auth]['n_papers'])\n",
    "        else:\n",
    "            phd_authors[phd_auth] = {}\n",
    "            phd_authors[phd_auth]['start_date'] = int(kk)\n",
    "            phd_authors[phd_auth]['n_papers'] = author_papers[phd_auth]['n_papers'].copy()\n",
    "            phd_authors[phd_auth]['gender'] = author_papers[phd_auth]['gender']\n",
    "    \n",
    "    \n",
    "    # Title length\n",
    "    titles = [vv['title'].split(' ') for vv in vals]\n",
    "    len_title = [len(tt) for tt in titles]\n",
    "    year_stats['len_title'].append(np.mean(len_title))\n",
    "    \n",
    "    abbrev_title = [y for x in titles for y in x if y.isupper() and y[-1] ==':']\n",
    "    year_stats['abbrev_title'].append(100 * len(abbrev_title) / len(titles))\n",
    "    \n",
    "    len_title = [1 if 'All You Need' in tt or 'all you need' in tt else 0 for tt in titles ]\n",
    "    year_stats['title_ayn'].append(np.mean(len_title))\n",
    "    \n",
    "    # Reviews\n",
    "    n_reviews = [len(vv['reviews']) for vv in vals if vv['reviews'] is not None]\n",
    "    n_avg_conf = [np.mean([rc['confidence'] for rc in vv['reviews'].values() if rc['confidence'] is not None]) for vv in vals if vv['reviews'] is not None]\n",
    "    n_avg_words = [np.mean([len(rc['text'].split(' ')) for rc in vv['reviews'].values()]) for vv in vals if vv['reviews'] is not None]\n",
    "    \n",
    "    year_stats['review_avg_num'].append(np.mean(n_reviews))\n",
    "    year_stats['review_avg_confidence'].append(np.mean(n_avg_conf))\n",
    "    year_stats['review_avg_length'].append(np.mean(n_avg_words))\n",
    "    \n",
    "    \n",
    "# Sorting\n",
    "total_institutions_sorted = [{w: total_institutions[w]} for w in sorted(total_institutions, key=total_institutions.get, reverse=False) if w is not None and w!='']\n",
    "total_authors_sorted = [{w: len(total_authors[w]['n_papers'])} for w in sorted(total_authors, key=lambda x: len(total_authors[x]['n_papers']), reverse=False)]\n",
    "\n",
    "\n",
    "# PhD analysis\n",
    "phd_years = {}\n",
    "for vv in phd_authors.values():\n",
    "    if vv['start_date'] in phd_years.keys():\n",
    "        phd_years[vv['start_date']].append(len(vv['n_papers']))\n",
    "    else:\n",
    "        phd_years[vv['start_date']] = [len(vv['n_papers'])]\n",
    "        \n",
    "phd_productivity = {}\n",
    "for yy in sorted(phd_years.keys()):\n",
    "    phd_productivity[yy] = np.mean(phd_years[yy])\n",
    "\n",
    "phd_numbers = {}\n",
    "for yy in sorted(phd_years.keys()):\n",
    "    phd_numbers[yy] = len(phd_years[yy])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([sum([pp[0]==1 for pp in vv['n_papers']]) for aa, vv in year_stats['unique_authors_info'][0].items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# country_papers = {}\n",
    "# for item in total_institutions_sorted:\n",
    "#     print(list(item.keys())[0])\n",
    "#     country = get_institution_country(list(item.keys())[0])\n",
    "#     if country in country_papers.keys():\n",
    "#         country_papers[country] += list(item.values())[0]\n",
    "#     else:\n",
    "#         country_papers[country] = list(item.values())[0]\n",
    "        \n",
    "# country_gdp = {}\n",
    "# data = pd.read_csv('GDP_data.csv', encoding='utf-8')\n",
    "# for cc in country_papers.keys():\n",
    "#     if len(data.index[data.iloc[:,0] == cc]):\n",
    "#         country_gdp[cc] = float(data.iloc[data.index[(data.iloc[:,0] == cc)], -2])\n",
    "#     else:\n",
    "#         print(\"No data for\", cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(list(total_institutions.values())))\n",
    "print(np.sum([len(vv['n_papers']) for vv in total_authors.values()]))\n",
    "print(np.sum([len(vv) for vv in conf_data.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative analysis\n",
    "\n",
    "In this section let's visualise the basic information, such as:\n",
    "\n",
    "- number of accepted papers (In the first conference instance in 1987 there was a total of 90 accepted papers, while in 2020 this number is 1898)\n",
    "\n",
    "\n",
    "- total number of unique authors (conference openess)\n",
    "- number of authors as a percentage of world population (perspective)\n",
    "- total number of unique institutions (conference openess)\n",
    "\n",
    "\n",
    "- average number of authors per paper (could be an indicator of paper complexity)\n",
    "- average number of different institutions per paper (institutional collaboration)\n",
    "\n",
    "\n",
    "- average number of papers per author\n",
    "- average number of papers per institution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(year_stats['n_accepted'])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Total accepted papers by year')\n",
    "plt.ylabel('# accepted papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(year_stats['unique_authors'])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Total unique authors at conference')\n",
    "plt.ylabel('# authors')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_stats['unique_institutions_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(len(conf_data)), year_stats['unique_institutions'])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Total unique institutions at conference')\n",
    "plt.ylabel('# institutions')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(year_stats['avg_authors_per_paper'])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average number of authors per paper')\n",
    "plt.ylabel('# authors')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(year_stats['avg_institutions_per_paper'])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average number of institutions per paper')\n",
    "plt.ylabel('# authors')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(year_stats['avg_papers_per_author'])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average number of papers per author')\n",
    "plt.ylabel('# papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(year_stats['avg_papers_per_institution'])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average number of papers per institution')\n",
    "plt.ylabel('# papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I would like to investigate more \n",
    "- [TODO] average number of citations per paper (although this is biased as older papers have been around more)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Quality of reviews:\n",
    "- number of reviews per paper\n",
    "- average reviewer confidence\n",
    "- average review length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(len(conf_data)), year_stats['review_avg_num'])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average number of reviewers per paper')\n",
    "plt.ylabel('# reviews')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(len(conf_data)), year_stats['review_avg_confidence'])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average reviewer confidence')\n",
    "plt.ylabel('confidence score')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(len(conf_data)), year_stats['review_avg_length'])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average review length')\n",
    "plt.ylabel('# words')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper title:\n",
    "- average title length\n",
    "- use of abbreviations in the title\n",
    "- \"All you need\" in the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(year_stats['len_title'])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average paper title length')\n",
    "plt.ylabel('# words')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(year_stats['abbrev_title'])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Papers with abbreviations in the title')\n",
    "plt.ylabel('% papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(year_stats['title_ayn'])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('\"...All You Need\" in title')\n",
    "plt.ylabel('% papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper Abstract:\n",
    "- average abstract length\n",
    "- average abstract word length\n",
    "- average abstract word complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper content:\n",
    "\n",
    "- average paper word length\n",
    "- average paper word complexity\n",
    "\n",
    "Although since the paper content has probably been parsed from a PDF it is possible that these numbers might be a bit off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplementary material\n",
    "- total number of papers containing some type of supplementary material (pdf or zip)\n",
    "- average number of papers containing some type of supplementary material (pdf or zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_supplementary)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Total papers containing supplementary material')\n",
    "plt.ylabel('# papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_supplementary)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Percentage of papers containing supplementary material')\n",
    "plt.ylabel('% of all papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation analysis\n",
    "[WARNING!] correlation != causation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trends analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- average number of papers within authors first 5 years (aimed at estimating PhD student \"productivity\")\n",
    "- average number of first-time authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(phd_productivity.values())\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title(\"Average number of >>first-author papers<< within author's first 5-year period\")\n",
    "plt.ylabel('# papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(phd_productivity)), sorted(phd_productivity.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(phd_numbers.values())\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title(\"Number first-time authors\")\n",
    "plt.ylabel('# authors')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(phd_numbers)), sorted(phd_numbers.keys()), rotation=-45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- number of papers containing an abbreviation (if there is at least one all-caps word in the title)\n",
    "\n",
    "- Most popular word / phrase - try different n-grams\n",
    "\n",
    "- Institution with most accepted papers - top X and show their ratios as area proportions - show in legend by color\n",
    "\n",
    "- Author with most accepted papers - same as institutions - conference Gini index - total amount of papers per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_papers = [list(vv.values())[0] for vv in total_authors_sorted][-50:]\n",
    "n_authors = [list(vv.keys())[0] for vv in total_authors_sorted][-50:]\n",
    "\n",
    "plt.bar(np.arange(len(n_papers)), n_papers)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Cumulative total papers by author')\n",
    "plt.ylabel('# of papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(n_authors)), n_authors, rotation=-90)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = [list(vv.values())[0] for vv in total_authors_sorted]\n",
    "share_real = np.cumsum(gg)\n",
    "share_ideal = np.cumsum(np.ones_like(share_real) * share_real[-1] / len(share_real))\n",
    "share_diff = np.sum(share_ideal - share_real)\n",
    "gini_coef = share_diff / np.sum(share_ideal)\n",
    "plt.plot(share_real)\n",
    "plt.plot(share_ideal)\n",
    "# plt.axis('')\n",
    "plt.title(\"Gini coefficient: {}\".format(gini_coef.round(2)))\n",
    "plt.gca().set_aspect(1/(share_ideal[-1]/len(share_ideal)), adjustable='box')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_papers = [list(vv.values())[0] for vv in total_institutions_sorted][-50:]\n",
    "n_institutions = [list(vv.keys())[0] for vv in total_institutions_sorted][-50:]\n",
    "\n",
    "plt.bar(np.arange(len(n_papers)), n_papers)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Cumulative total papers by institution')\n",
    "plt.ylabel('# of papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(n_institutions)), n_institutions, rotation=-90)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = [list(vv.values())[0] for vv in total_institutions_sorted]\n",
    "share_real = np.cumsum(gg)\n",
    "share_ideal = np.cumsum(np.ones_like(share_real) * share_real[-1] / len(share_real))\n",
    "share_diff = np.sum(share_ideal - share_real)\n",
    "gini_coef = share_diff / np.sum(share_ideal)\n",
    "plt.plot(share_real)\n",
    "plt.plot(share_ideal)\n",
    "plt.title(\"Gini coefficient: {}\".format(gini_coef.round(2)))\n",
    "plt.gca().set_aspect(1/(share_ideal[-1]/len(share_ideal)), adjustable='box')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 10)\n",
    "# http://kimberlyfessel.com/mathematics/applications/gini-use-cases/\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inclusivity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Country's GDP vs number of papers\n",
    "\n",
    "This is a very rough approximate as country data was not given, and I tried to obtain country data from unstructured Institution names. This procedure consists of checking Location identifier on Wikipedia page of a given institution name. A better option was Google's answer box when searching \"INSTITUTION_NAME country name\" however, I was unable to access this information, and SERPAPI Google's API for scraping is not free.\n",
    "\n",
    "Regarding GDP info, I downloaded the spreadsheet file from https://databank.worldbank.org/data/download/GDP.xls and parsed the data for the exctracted countries for 2019.\n",
    "\n",
    "As expected we can see a slight positive correlation between the country's GDP and the paper output its institutions produced. To quantify the correlation, I calculated the Pearson's correlation coefficient ($\\rho = 0.7$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('country_gdp.pkl', 'rb') as handle:\n",
    "    country_gdp = pickle.load(handle)\n",
    "with open('country_papers.pkl', 'rb') as handle:\n",
    "    country_papers = pickle.load(handle)\n",
    "    \n",
    "data_gdp = []\n",
    "data_paper = []\n",
    "\n",
    "# Plot datapoints\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 10)\n",
    "NUM_COLORS = len(country_papers)\n",
    "cm = plt.get_cmap('tab20')\n",
    "ax.set_prop_cycle('color', [cm(i % 20) for i in range(NUM_COLORS)])\n",
    "for i, c_names in enumerate(country_papers.keys()):\n",
    "    if c_names in country_gdp.keys():\n",
    "        data_gdp.append(country_gdp[c_names])\n",
    "        data_paper.append(country_papers[c_names])\n",
    "        ax.scatter(country_gdp[c_names], country_papers[c_names], s=50, label=c_names, marker='o' if i<20 else 'p')\n",
    "# Calculate the pearson's correlation coefficient\n",
    "\n",
    "# Plot\n",
    "ax.set_xlabel('GDP [$]')\n",
    "ax.set_ylabel('# papers')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.legend(ncol=1, loc='upper left', bbox_to_anchor=(1, 0., 1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ratio of author sex over the years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this is an important indicator of diversity, such author information is not available from the NeurIPS paper website. It is possible to get a very rough estimate based on the author's given name by comparing it to a name database. Similarly to the GDP results, this should be taken with a grain of salt, as some of the given names which are abbreviated or ambiguous have been classified as 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
