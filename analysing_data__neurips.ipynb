{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeurIPS conference  -  historical data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conferences often provide statistics for a particular year, presenting information such as acceptance rate, diversity, geographical and topic distribution, etc. Usually for only that year, or perhaps up to a few years back.\n",
    "\n",
    "However, it might be useful to take a step back and see the big picture, in order to visualise some trends and gain useful insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the focus on the NeurIPS conference data. The accepted papers from all conference instances since 1987, when it was first held, are available on https://papers.nips.cc/. Unfortunately, there would be also many insights from rejected papers, but these are not available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the organisers of NeurIPS, all the accepted paper data is neatly organised onine, with the following information available:\n",
    "- title\n",
    "- year\n",
    "- author names \n",
    "- suplementary material indicator (yes/no)\n",
    "- [some] author affiliation\n",
    "- [some] abstract\n",
    "- [some] full paper text contents\n",
    "- [some] reviews\n",
    "- [TODO] number of citations (scraped from scholar)\n",
    "\n",
    "\n",
    "In addition to this, I tried certain approaches  to extract more data which is not directly given on the main website. All the source code I used to obtain the data and generate the plots is on my GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first used the `scripts/neurips_download.py` to scrape all the data from the website. Then I extract and plot interesting data as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # install packages if necessary\n",
    "# ! pip install nltk\n",
    "# ! pip install requests\n",
    "# ! pip install beautifulsoup4\n",
    "# ! pip install gender-guesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import requests\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import gender_guesser.detector as gender\n",
    "detector = gender.Detector()\n",
    "import scipy.stats as stats\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('data/neurips_conf_data.pkl', 'rb') as handle:\n",
    "    conf_data = pickle.load(handle)\n",
    "    \n",
    "total_authors = {}\n",
    "total_institutions = {}\n",
    "phd_authors = {}\n",
    "year_stats = {}\n",
    "\n",
    "year_stats['unique_authors_info'] = []\n",
    "year_stats['unique_institutions_info'] = []\n",
    "\n",
    "for kk, vals in conf_data.items():\n",
    "    # Unique authors and institutions\n",
    "    paper_info = {}\n",
    "    author_papers = {}\n",
    "    institution_papers = {}\n",
    "    for paper in vals:\n",
    "        # get yearly unique authors and papers per author\n",
    "        for a_idx, a_name in enumerate(paper['authors']):\n",
    "            ahash = '{} {}'.format(a_name['given_name'], a_name['family_name'])  # ahash = '{}_{}'.format(a_name['given_name'].upper(), a_name['family_name'].upper())\n",
    "            if ahash in author_papers.keys():\n",
    "                author_papers[ahash]['n_papers'].append((a_idx + 1, len(paper['authors'])))\n",
    "            else:\n",
    "                author_papers[ahash] = {}\n",
    "                author_papers[ahash]['n_papers'] = [(a_idx + 1, len(paper['authors']))]\n",
    "                author_papers[ahash]['gender'] = detector.get_gender(ahash.split()[0])\n",
    "        # get yearly unique institutions and papers per institution\n",
    "        for i_name in paper['institutions']:\n",
    "            if i_name is None or i_name == '':\n",
    "                continue\n",
    "            if i_name in institution_papers.keys():\n",
    "                institution_papers[i_name] += 1\n",
    "            else:\n",
    "                institution_papers[i_name] = 1\n",
    "    # Update yearly author and institution info\n",
    "    year_stats['unique_authors_info'].append(author_papers)\n",
    "    year_stats['unique_institutions_info'].append(institution_papers)\n",
    "    \n",
    "    # Update total authors and institutions\n",
    "    for t_author in author_papers.keys():\n",
    "        if t_author in total_authors.keys():\n",
    "            total_authors[t_author]['n_papers'].extend(author_papers[t_author]['n_papers'])\n",
    "        else:\n",
    "            total_authors[t_author] = {}\n",
    "            total_authors[t_author]['n_papers'] = author_papers[t_author]['n_papers'].copy()\n",
    "            total_authors[t_author]['gender'] = author_papers[t_author]['gender']\n",
    "            \n",
    "    for t_inst in institution_papers.keys():\n",
    "        if t_inst in total_institutions.keys():\n",
    "            total_institutions[t_inst] += institution_papers[t_inst]\n",
    "        else:\n",
    "            total_institutions[t_inst] = institution_papers[t_inst]\n",
    "            \n",
    "    # Update PhD author productivity\n",
    "    for phd_auth in author_papers.keys():\n",
    "        if phd_auth in phd_authors.keys():\n",
    "            if int(kk) < phd_authors[phd_auth]['start_date'] + 5:\n",
    "                phd_authors[phd_auth]['n_papers'].extend(author_papers[phd_auth]['n_papers'])\n",
    "        else:\n",
    "            phd_authors[phd_auth] = {}\n",
    "            phd_authors[phd_auth]['start_date'] = int(kk)\n",
    "            phd_authors[phd_auth]['n_papers'] = author_papers[phd_auth]['n_papers'].copy()\n",
    "            phd_authors[phd_auth]['gender'] = author_papers[phd_auth]['gender']\n",
    "    \n",
    "    \n",
    "# Sorting by number of papers\n",
    "total_institutions_sorted = [{w: total_institutions[w]} for w in sorted(total_institutions, key=total_institutions.get, reverse=False) if w is not None and w != '']\n",
    "total_authors_sorted = [{w: len(total_authors[w]['n_papers'])} for w in sorted(total_authors, key=lambda x: len(total_authors[x]['n_papers']), reverse=False)]\n",
    "\n",
    "\n",
    "# PhD analysis\n",
    "phd_start_year = {}\n",
    "for kk, vv in phd_authors.items():\n",
    "    if vv['start_date'] in phd_start_year.keys():\n",
    "        phd_start_year[vv['start_date']].append(vv['n_papers'])\n",
    "    else:\n",
    "        phd_start_year[vv['start_date']] = [vv['n_papers']]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first start with the total number of accepted papers. As the number of submissions is not available, we cannot monitor the acceptance rate. Still, the number of submissions drastically increased in the recent years:\n",
    "\n",
    "In the first conference instance, in 1987, there was a total of 90 accepted papers, while in 2020 this number is 1898. The overall number of accepted papers over the years is 11578."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_accepted = [len(conf_data[kk]) for kk in sorted(conf_data.keys())]\n",
    "plt.scatter(np.arange(len(n_accepted)), n_accepted)\n",
    "plt.plot(n_accepted)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Total accepted papers by year')\n",
    "plt.ylabel('# accepted papers')\n",
    "plt.ylim(0)\n",
    "plt.xlim(-1.5, 35)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--')\n",
    "plt.savefig('plots/fig__total_accepted.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for the next year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_accepted = [len(conf_data[kk]) for kk in sorted(conf_data.keys())]\n",
    "x_vals = np.arange(len(n_accepted))\n",
    "\n",
    "\n",
    "poly_deg = 10\n",
    "z_fn = np.poly1d(np.polyfit(x_vals, n_accepted, poly_deg))\n",
    "pred_accepted = z_fn(np.arange(len(n_accepted) + 1))\n",
    "\n",
    "plt.plot(n_accepted, linewidth=3)\n",
    "plt.scatter(x_vals, n_accepted)\n",
    "\n",
    "plt.plot(pred_accepted, alpha=1, linestyle='-')\n",
    "# plt.scatter(np.arange(len(n_accepted) + 1), pred_accepted)\n",
    "plt.scatter(len(n_accepted), pred_accepted[-1], s=50)\n",
    "pred_2021 = int(pred_accepted[-1])\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Total accepted papers by year, with prediction for 2021')\n",
    "plt.ylabel('# accepted papers')\n",
    "plt.text(32.1, pred_2021 - 10, str(pred_2021), size=12, color='C1', fontweight='bold')\n",
    "\n",
    "plt.ylim(0)\n",
    "plt.xlim(-1.5, 35)\n",
    "plt.xticks(np.arange(len(conf_data)+1), list(sorted(conf_data.keys())) + ['2021'], rotation=-45)\n",
    "plt.grid(linestyle='--')\n",
    "plt.savefig('plots/fig__pred_accepted.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further insights can be obtained from the paper title:\n",
    "- Average title length\n",
    "- Percentage of papers containing an abbreviation (if there is at least one all-caps word in the title)\n",
    "- \"... All you need\" in the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_lenghts = np.array([np.mean([len(vv['title'].split(' ')) for vv in conf_data[kk]]) for kk in sorted(conf_data.keys())])\n",
    "title_lenghts_std = np.array([np.std([len(vv['title'].split(' ')) for vv in conf_data[kk]]) for kk in sorted(conf_data.keys())])\n",
    "plt.plot(title_lenghts, color='C0')\n",
    "plt.fill_between(\n",
    "    np.arange(len(conf_data)),\n",
    "    title_lenghts - 0.5 * title_lenghts_std,\n",
    "    title_lenghts + 0.5 * title_lenghts_std,\n",
    "    alpha=0.25, color='C0')\n",
    "plt.scatter(np.arange(len(title_lenghts)), title_lenghts, color='C0')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average paper title length')\n",
    "plt.ylabel('# words')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--')\n",
    "# plt.savefig('plots/fig__title_length.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_abbreviations = [100 * np.mean([sum([ww.isupper() and ww[-1] == ':' for ww in vv['title'].split(' ')])\n",
    "                                   for vv in conf_data[kk]])\n",
    "                        for kk in sorted(conf_data.keys())]\n",
    "\n",
    "plt.bar(np.arange(len(titles_abbreviations)), titles_abbreviations)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Papers with an abbreviation in the title')\n",
    "plt.ylabel('% papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "# plt.savefig('plots/fig__title_abbreviations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ayn = [sum([1 if 'All You Need' in vv['title'] or 'all you need' in vv['title'] else 0 for vv in conf_data[kk]]) for kk in sorted(conf_data.keys())]\n",
    "plt.bar(np.arange(len(title_ayn)), title_ayn)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Papers with \"...All You Need\" in title')\n",
    "plt.ylabel('# papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "# plt.savefig('plots/fig__title_ayn.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common words and phrases in the title, that show up over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up titles\n",
    "nltk.download('stopwords')\n",
    "stop_words = list(set(nltk.corpus.stopwords.words('english'))) \n",
    "titles_no_stopwords = [[' '.join([ww.replace(\"\\'\",'') for ww in pp['title'].lower().split() if ww not in stop_words]) for pp in conf_data[kk]] for kk in sorted(conf_data.keys())]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalising by the number of accepted paper, shows a different picture than not doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_accepted = np.array([len(conf_data[kk]) for kk in sorted(conf_data.keys())])\n",
    "# n_accepted = np.ones(len(conf_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find most common single words\n",
    "title_words_by_year = [[tt.split() for tt in yy] for yy in titles_no_stopwords]\n",
    "title_words_by_year = [[y for x in oo for y in x] for oo in title_words_by_year]\n",
    "common_words = [dict(collections.Counter(tt).most_common(10)) for tt in title_words_by_year]\n",
    "\n",
    "# Extract most common 1gram phrase over the years\n",
    "most_common_1grams = {}\n",
    "for i, yy in enumerate(common_words):\n",
    "    for kk in yy.keys():\n",
    "        if kk in most_common_1grams.keys():\n",
    "            most_common_1grams[kk][i] = yy[kk] / n_accepted[i]\n",
    "        elif kk[-1] == 's' and kk[:-1] in most_common_1grams.keys():\n",
    "            most_common_1grams[kk[:-1]][i] = yy[kk] / n_accepted[i]\n",
    "        elif kk[-1] != 's' and kk+'s' in most_common_1grams.keys():\n",
    "            most_common_1grams[kk+'s'][i] = yy[kk] / n_accepted[i]\n",
    "        else:\n",
    "            most_common_1grams[kk] = np.zeros(len(common_words))\n",
    "# Cleanup\n",
    "del most_common_1grams['via']\n",
    "del most_common_1grams['using']\n",
    "# Sort by number of mentions\n",
    "sorted_1grams = collections.OrderedDict()\n",
    "for k in sorted(most_common_1grams.keys(), key=lambda item: np.sum(most_common_1grams[item]), reverse=True):\n",
    "    sorted_1grams[k] = most_common_1grams[k]\n",
    "\n",
    "# Plot top N phrases\n",
    "N = 5\n",
    "OFFSET = 0.02\n",
    "for i, (ph, fq) in enumerate(list(sorted_1grams.items())[:N]):\n",
    "#     plt.plot(fq, label=ph, lw=5)\n",
    "    plt.bar(np.arange(len(n_accepted)) + (i+1 - N/2)/(N+OFFSET*10), fq, width=1/(N+OFFSET), label=ph)\n",
    "    \n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16, 8)\n",
    "plt.title('Most frequent words')\n",
    "plt.ylabel('normalized mentions')\n",
    "plt.ylim(0)\n",
    "plt.xlim(-0.9, 33.9)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "# plt.grid(linestyle='--')\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "lgd = plt.legend(ncol=N, loc='center', bbox_to_anchor=(0.5, 0.96, 0, 0))\n",
    "# plt.savefig('plots/fig__title_1gram.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common 2-word phrases words\n",
    "title_2grams_by_year = [[[' '.join(ww) for ww in nltk.ngrams(pp.split(), 2)] for pp in yy] for yy in titles_no_stopwords]\n",
    "title_2grams_by_year = [[y for x in oo for y in x] for oo in title_2grams_by_year]\n",
    "common_2grams = [dict(collections.Counter(tt).most_common(10)) for tt in title_2grams_by_year]\n",
    "\n",
    "# Extract most common 2gram phrase over the years\n",
    "most_common_2grams = {}\n",
    "for i, yy in enumerate(common_2grams):\n",
    "    for kk in yy.keys():\n",
    "        if kk in most_common_2grams.keys():\n",
    "            most_common_2grams[kk][i] = yy[kk] / n_accepted[i]\n",
    "        elif kk[-1] == 's' and kk[:-1] in most_common_2grams.keys():\n",
    "            most_common_2grams[kk[:-1]][i] = yy[kk] / n_accepted[i]\n",
    "        elif kk[-1] != 's' and kk+'s' in most_common_2grams.keys():\n",
    "            most_common_2grams[kk+'s'][i] = yy[kk] / n_accepted[i]\n",
    "        else:\n",
    "            most_common_2grams[kk] = np.zeros(len(common_2grams))\n",
    "# Sort by number of mentions\n",
    "sorted_2grams = collections.OrderedDict()\n",
    "for k in sorted(most_common_2grams.keys(), key=lambda item: np.sum(most_common_2grams[item]), reverse=True):\n",
    "    sorted_2grams[k] = most_common_2grams[k]\n",
    "    \n",
    "# Plot top N phrases\n",
    "N = 5\n",
    "OFFSET = 0.02\n",
    "for i, (ph, fq) in enumerate(list(sorted_2grams.items())[:N]):\n",
    "#     plt.plot(fq, label=ph, lw=3)\n",
    "    plt.bar(np.arange(len(n_accepted)) + (i+1 - N/2)/(N+OFFSET * 10), fq, width=1/(N+OFFSET), label=ph)\n",
    "    \n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16, 8)\n",
    "plt.title('Most frequent 2-word phrases')\n",
    "plt.ylabel('normalized mentions')\n",
    "plt.ylim(0, .13)\n",
    "plt.xlim(-0.9, 33.9)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "# plt.grid(linestyle='--')\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "lgd = plt.legend(ncol=N, loc='center', bbox_to_anchor=(0.5, 0.96, 0, 0))\n",
    "# plt.savefig('plots/fig__title_2gram.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common 3-word phrases words\n",
    "title_3grams_by_year = [[[' '.join(ww) for ww in nltk.ngrams(pp.split(), 3)] for pp in yy] for yy in titles_no_stopwords]\n",
    "title_3grams_by_year = [[y for x in oo for y in x] for oo in title_3grams_by_year]\n",
    "common_3grams = [dict(collections.Counter(tt).most_common(10)) for tt in title_3grams_by_year]\n",
    "\n",
    "# Extract most common 3gram phrase over the years\n",
    "most_common_3grams = {}\n",
    "for i, yy in enumerate(common_3grams):\n",
    "    for kk in yy.keys():\n",
    "        if kk in most_common_3grams.keys():\n",
    "            most_common_3grams[kk][i] = yy[kk] / n_accepted[i]\n",
    "        elif kk[-1] == 's' and kk[:-1] in most_common_3grams.keys():\n",
    "            most_common_3grams[kk[:-1]][i] = yy[kk] / n_accepted[i]\n",
    "        elif kk[-1] != 's' and kk+'s' in most_common_3grams.keys():\n",
    "            most_common_3grams[kk+'s'][i] = yy[kk] / n_accepted[i]\n",
    "        else:\n",
    "            most_common_3grams[kk] = np.zeros(len(common_3grams))\n",
    "# Sort by number of mentions\n",
    "sorted_3grams = collections.OrderedDict()\n",
    "for k in sorted(most_common_3grams.keys(), key=lambda item: np.sum(most_common_3grams[item]), reverse=True):\n",
    "    sorted_3grams[k] = most_common_3grams[k]\n",
    "    \n",
    "# Plot top N phrases\n",
    "N = 6\n",
    "OFFSET = 0.02\n",
    "for i, (ph, fq) in enumerate(list(sorted_3grams.items())[:N]):\n",
    "#     plt.plot(fq, label=ph, lw=3)\n",
    "    plt.bar(np.arange(len(n_accepted)) + (i+1 - N/2)/(N+OFFSET*10), fq, width=1/(N+OFFSET), label=ph)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Most frequent 3-word phrases')\n",
    "plt.ylabel('normalized mentions')\n",
    "plt.ylim(0)\n",
    "plt.xlim(-0.9, 33.9)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "# plt.grid(linestyle='--')\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "lgd = plt.legend(ncol=N//3, loc='upper right', bbox_to_anchor=(1, 1, 0, 0))\n",
    "# plt.savefig('plots/fig__title_3gram.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, some insights from the paper abstract:\n",
    "- Average abstract length\n",
    "- Average abstract word length\n",
    "- Average abstract word complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_lenghts = np.array([np.mean([len(vv['abstract'].split(' ')) for vv in conf_data[kk] if vv['abstract'] is not None]) for kk in sorted(conf_data.keys())])\n",
    "abstract_lenghts_std = np.array([np.std([len(vv['abstract'].split(' ')) for vv in conf_data[kk] if vv['abstract'] is not None]) for kk in sorted(conf_data.keys())])\n",
    "plt.bar(np.arange(len(abstract_lenghts)), abstract_lenghts, yerr=abstract_lenghts_std)\n",
    "\n",
    "#plt.plot(abstract_lenghts)\n",
    "# plt.fill_between(\n",
    "#     np.arange(len(conf_data)),\n",
    "#     abstract_lenghts - 0.5 * abstract_lenghts_std,\n",
    "#     abstract_lenghts + 0.5 * abstract_lenghts_std,\n",
    "#     alpha=0.25)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average paper abstract length')\n",
    "plt.ylabel('# words')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "# plt.savefig('plots/fig__abstract_length.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_word_len = np.array([np.mean([np.mean([len(ww) for ww in vv['abstract'].replace('\\n', ' ').split(' ') if len(ww) > 3]) for vv in conf_data[kk] if vv['abstract'] is not None and vv['abstract'] != '']) for kk in sorted(conf_data.keys())])\n",
    "abstract_word_len_std = np.array([np.std([np.mean([len(ww) for ww in vv['abstract'].replace('\\n', ' ').split(' ') if len(ww) > 3]) for vv in conf_data[kk] if vv['abstract'] is not None and vv['abstract'] != '']) for kk in sorted(conf_data.keys())])\n",
    "\n",
    "plt.bar(np.arange(len(abstract_word_len)), abstract_word_len, yerr=abstract_word_len_std)\n",
    "# plt.plot(abstract_word_len)\n",
    "# plt.fill_between(\n",
    "#     np.arange(len(conf_data)),\n",
    "#     abstract_word_len - 0.5 * abstract_word_len_std,\n",
    "#     abstract_word_len + 0.5 * abstract_word_len_std,\n",
    "#     alpha=0.25)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average paper abstract word length')\n",
    "plt.ylabel('# characters')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "# plt.savefig('plots/fig__abstract_words.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicators of papers containing supplemental material:\n",
    "- Total number of papers containing some type of supplementary material (pdf or zip)\n",
    "- Average number of papers containing some type of supplementary material (pdf or zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_supplemental = [np.sum([vv['has_supplement'] for vv in conf_data[kk]]) for kk in sorted(conf_data.keys())]\n",
    "plt.scatter(np.arange(len(total_supplemental)), total_supplemental)\n",
    "plt.plot(total_supplemental)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Total papers containing supplementary material')\n",
    "plt.ylabel('# papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--')\n",
    "# plt.savefig('plots/fig__supplemental_total.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_supplemental = [np.mean([vv['has_supplement'] for vv in conf_data[kk]]) * 100 for kk in sorted(conf_data.keys())]\n",
    "# plt.plot(percent_supplemental)\n",
    "plt.bar(np.arange(len(percent_supplemental)), percent_supplemental)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Percentage of papers containing supplementary material')\n",
    "plt.ylabel('% of all papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "# plt.savefig('plots/fig__supplemental_percent.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding author information, quick disclaimer, there are some inconsistencies in the names as they are sometimes abbreviated e.g. \"J.\", \"Josh\" or \"Joshua\". Although it is difficult to fix all such instances manually due to the large number of authors, we still have a good enough estimate.\n",
    "\n",
    "In the first year there were 166 authors participating, while in the last conference 5917 authors, leading to an overall total of 17670 participants over 34 conference instances.\n",
    "\n",
    "The total number of individuals participating each year are shown below, and it closely follows the number of accepted papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_authors = [len(ss) for ss in year_stats['unique_authors_info']]\n",
    "plt.scatter(np.arange(len(unique_authors)), unique_authors)\n",
    "plt.plot(unique_authors)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Total unique authors at conference')\n",
    "plt.ylabel('# authors')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--')\n",
    "# plt.savefig('plots/fig__unique_authors.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might also be interesting to look at the actual world population and compare against this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/world_population.pkl', 'rb') as handle:\n",
    "    world_population = pickle.load(handle)\n",
    "    \n",
    "pop_global = np.array([world_population[int(yy)]['pop_global'] for yy in sorted(conf_data.keys())])\n",
    "pop_urban = np.array([world_population[int(yy)]['pop_urban'] for yy in sorted(conf_data.keys())])\n",
    "# world\n",
    "plt.scatter(np.arange(len(pop_global)), pop_global)\n",
    "plt.plot(pop_global, label='global')\n",
    "# urban\n",
    "plt.scatter(np.arange(len(pop_urban)), pop_urban)\n",
    "plt.plot(pop_urban, label='urban')\n",
    "    \n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Total world population')\n",
    "plt.ylabel('Population size')\n",
    "# plt.ylim(0, 0.00015)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--')\n",
    "lgd = plt.legend(ncol=1, loc='upper left', bbox_to_anchor=(1, 0., 1, 1))\n",
    "# plt.savefig('plots/fig__world_population.png', dpi=300, bbox_inches='tight', bbox_extra_artists=(lgd,))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/world_population.pkl', 'rb') as handle:\n",
    "    world_population = pickle.load(handle)\n",
    "\n",
    "unique_authors = np.array([len(ss) for ss in year_stats['unique_authors_info']])\n",
    "pop_global = np.array([world_population[int(yy)]['pop_global'] for yy in sorted(conf_data.keys())])\n",
    "pop_urban = np.array([world_population[int(yy)]['pop_urban'] for yy in sorted(conf_data.keys())])\n",
    "# world\n",
    "authors_global = 100 * unique_authors / pop_global\n",
    "plt.scatter(np.arange(len(authors_global)), authors_global)\n",
    "plt.plot(authors_global, label='global')\n",
    "# urban\n",
    "authors_urban = 100 * unique_authors / pop_urban\n",
    "plt.scatter(np.arange(len(authors_urban)), authors_urban)\n",
    "plt.plot(authors_urban, label='urban')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Percentage of world population with accepted papers')\n",
    "plt.ylabel('% of world population')\n",
    "plt.ylim(0, 0.00015)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--')\n",
    "lgd = plt.legend(ncol=1, loc='upper left', bbox_to_anchor=(1, 0., 1, 1))\n",
    "# plt.savefig('plots/fig__world_precent.png', dpi=300, bbox_inches='tight', bbox_extra_artists=(lgd,))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_per_author_mean = np.array([np.mean([len(vv['n_papers']) for vv in ss.values()]) for ss in year_stats['unique_authors_info']])\n",
    "papers_per_author_std = np.array([np.mean([len(vv['n_papers']) for vv in ss.values()]) for ss in year_stats['unique_authors_info']])\n",
    "plt.plot(papers_per_author_mean, color='C0')\n",
    "plt.fill_between(\n",
    "    np.arange(len(conf_data)),\n",
    "    papers_per_author_mean - 0.5 * papers_per_author_std,\n",
    "    papers_per_author_mean + 0.5 * papers_per_author_std,\n",
    "    alpha=0.25, color='C0')\n",
    "plt.scatter(np.arange(len(papers_per_author_mean)), papers_per_author_mean, color='C0')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average number of papers per author')\n",
    "plt.ylabel('# papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--')\n",
    "# plt.savefig('plots/fig__papers_per_author.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 50 publishing authors, with most accepted papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_papers = [list(vv.values())[0] for vv in total_authors_sorted][-50:]\n",
    "n_authors = [list(vv.keys())[0] for vv in total_authors_sorted][-50:]\n",
    "\n",
    "plt.bar(np.arange(len(n_papers)), n_papers)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Cumulative total papers by author')\n",
    "plt.ylabel('# of papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(n_authors)), n_authors, rotation=-90)\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "# plt.savefig('plots/fig__all_authors.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate the actual inequality of the distribution of papers per author, one can estimate the Gini coefficient. The blue part in the graph below shows the cumulative number of publications, once we sort the authors by their number of publications. If each author had the same number of publications, the blue area would cover the orange area as well (which is the ideal equal distribution). Therefore, the ratio: (orange area) / (orange + blue area) gives us the Gini coefficient. The value of 0 indicates total equality (no orange area), while 1 indicates total inequality (all papers are published by a single author)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = [list(vv.values())[0] for vv in total_authors_sorted]\n",
    "share_real = np.cumsum(gg)\n",
    "share_ideal = np.cumsum(np.ones_like(share_real) * share_real[-1] / len(share_real))\n",
    "share_diff = np.sum(share_ideal - share_real)\n",
    "gini_coef = share_diff / np.sum(share_ideal)\n",
    "plt.fill_between(np.arange(len(total_authors_sorted)), 0, share_real)\n",
    "plt.fill_between(np.arange(len(total_authors_sorted)), share_real, share_ideal)\n",
    "# plt.axis('')\n",
    "plt.title(\"Gini coefficient: {}\".format(gini_coef.round(2)))\n",
    "plt.gca().set_aspect(1/(share_ideal[-1]/len(share_ideal)), adjustable='box')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 8)\n",
    "# plt.savefig('plots/fig__all_authors__gini.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although author diversity is an important aspect, information such as author gender diversity is not available from the NeurIPS paper website. It is possible to get a very rough estimate using a python library, which is based on comparing the author's given name to a name database (however mostly \"western\" names). The \"unknown\" label is mostly assigned to non-western names, or cases where the author's given name is abbreviated.\n",
    "\n",
    "Below are the graphs showing proportion of all papers and last-author papers, published by each author group. The last author is usually a more senior academic, such as a group leader or a supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_authors = np.array([len(ss) for ss in year_stats['unique_authors_info']])\n",
    "n_female = np.array([np.sum(['female' in vv['gender'] for vv in ss.values()]) for ss in year_stats['unique_authors_info']])\n",
    "n_female = n_female / n_authors\n",
    "n_male = np.array([np.sum([vv['gender'] == 'male' or vv['gender'] == 'mostly_male' for vv in ss.values()]) for ss in year_stats['unique_authors_info']])\n",
    "n_male = n_male / n_authors\n",
    "n_unknown = np.array([np.sum(['male' not in vv['gender'] for vv in ss.values()]) for ss in year_stats['unique_authors_info']])\n",
    "n_unknown = n_unknown / n_authors\n",
    "\n",
    "n_years = np.arange(len(year_stats['unique_authors_info']))\n",
    "plt.fill_between(n_years, n_unknown + n_male, n_unknown + n_male + n_female, label='female')\n",
    "plt.fill_between(n_years, n_unknown, n_unknown + n_male, label='male')\n",
    "plt.fill_between(n_years, 0, n_unknown, label='unknown')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Estimated author gender distribution per year')\n",
    "plt.ylabel('proportion of papers')\n",
    "plt.ylim(-0.05)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "lgd = plt.legend(ncol=1, loc='upper left', bbox_to_anchor=(1, 0., 1, 1))\n",
    "plt.grid(linestyle='--', color='w', axis='y')\n",
    "# plt.savefig('plots/fig__genders_total.png', dpi=300, bbox_inches='tight', bbox_extra_artists=(lgd,))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as n_accepted\n",
    "n_accepted = np.array([np.sum([np.sum([pp[0] == 1 for pp in vv['n_papers']]) for vv in ss.values()]) for ss in year_stats['unique_authors_info']])\n",
    "\n",
    "n_first_female = np.array([np.sum([np.sum([pp[0] == 1 for pp in vv['n_papers']]) for vv in ss.values() if 'female' in vv['gender']]) for ss in year_stats['unique_authors_info']])\n",
    "n_first_female = n_first_female / n_accepted\n",
    "\n",
    "n_first_male = np.array([np.sum([np.sum([pp[0] == 1 for pp in vv['n_papers']]) for vv in ss.values() if (vv['gender'] == 'male' or vv['gender'] == 'mostly_male')]) for ss in year_stats['unique_authors_info']])\n",
    "n_first_male = n_first_male / n_accepted\n",
    "\n",
    "n_first_unknown = np.array([np.sum([np.sum([pp[0] == 1 for pp in vv['n_papers']]) for vv in ss.values() if 'male' not in vv['gender']]) for ss in year_stats['unique_authors_info']])\n",
    "n_first_unknown = n_first_unknown / n_accepted\n",
    "\n",
    "n_years = np.arange(len(year_stats['unique_authors_info']))\n",
    "plt.fill_between(n_years, n_first_unknown + n_first_male, n_first_unknown + n_first_male + n_first_female, label='female')\n",
    "plt.fill_between(n_years, n_first_unknown, n_first_unknown + n_first_male, label='male')\n",
    "plt.fill_between(n_years, 0, n_first_unknown, label='unknown')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Estimated first-author gender distribution per year')\n",
    "plt.ylabel('proportion of papers')\n",
    "plt.ylim(-0.05)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "lgd = plt.legend(ncol=1, loc='upper left', bbox_to_anchor=(1, 0., 1, 1))\n",
    "plt.grid(linestyle='--', color='w', axis='y')\n",
    "# plt.savefig('plots/fig__genders_first.png', dpi=300, bbox_inches='tight', bbox_extra_artists=(lgd,))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as n_accepted\n",
    "n_accepted = np.array([np.sum([np.sum([pp[0] == pp[1] for pp in vv['n_papers']]) for vv in ss.values()]) for ss in year_stats['unique_authors_info']])\n",
    "\n",
    "n_first_female = np.array([np.sum([np.sum([pp[0] == pp[1] for pp in vv['n_papers']]) for vv in ss.values() if 'female' in vv['gender']]) for ss in year_stats['unique_authors_info']])\n",
    "n_first_female = n_first_female / n_accepted\n",
    "\n",
    "n_first_male = np.array([np.sum([np.sum([pp[0] == pp[1] for pp in vv['n_papers']]) for vv in ss.values() if (vv['gender'] == 'male' or vv['gender'] == 'mostly_male')]) for ss in year_stats['unique_authors_info']])\n",
    "n_first_male = n_first_male / n_accepted\n",
    "\n",
    "n_first_unknown = np.array([np.sum([np.sum([pp[0] == pp[1] for pp in vv['n_papers']]) for vv in ss.values() if 'male' not in vv['gender']]) for ss in year_stats['unique_authors_info']])\n",
    "n_first_unknown = n_first_unknown / n_accepted\n",
    "\n",
    "n_years = np.arange(len(year_stats['unique_authors_info']))\n",
    "plt.fill_between(n_years, n_first_unknown + n_first_male, n_first_unknown + n_first_male + n_first_female, label='female')\n",
    "plt.fill_between(n_years, n_first_unknown, n_first_unknown + n_first_male, label='male')\n",
    "plt.fill_between(n_years, 0, n_first_unknown, label='unknown')\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Estimated last-author gender distribution per year')\n",
    "plt.ylabel('proportion of papers')\n",
    "plt.ylim(-0.05)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "lgd = plt.legend(ncol=1, loc='upper left', bbox_to_anchor=(1, 0., 1, 1))\n",
    "plt.grid(linestyle='--', color='w', axis='y')\n",
    "# plt.savefig('plots/fig__genders_last.png', dpi=300, bbox_inches='tight', bbox_extra_artists=(lgd,))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"PhD\" author statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I was interested in estimating the number of publications by PhD students over the years. Even though there is no way to reliably estimate the number of PhD students, and which authors are PhD students, what we can do is to check how many papers were published by each author, within the first 5 years since their first publication at this conference.\n",
    "\n",
    "Both the total amount of papers and first-author papers are mostly constant, with the trend going upwards, which could be expected.\n",
    "\n",
    "The graphs below show:\n",
    "- average number of papers published within author's first 5 years.\n",
    "- average number of __first author__ papers published within author's first 5 years, excluding single-author papers (not considering equal contributions, as there is no such information. If one autor had N first-author papers in one year, this author's estimated gender is counted N times.)\n",
    "- average number of __last author__ papers published within author's first 5 years (probably not their first time publishing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phd_productivity = np.array([np.mean([len(aa) for aa in yy]) for yy in phd_start_year.values()])[:-5]\n",
    "phd_productivity_std = np.array([np.std([len(aa) for aa in yy]) for yy in phd_start_year.values()])[:-5]\n",
    "plt.plot(phd_productivity, color='C0')\n",
    "plt.fill_between(\n",
    "    np.arange(len(phd_productivity)),\n",
    "    phd_productivity - 0.5 * phd_productivity_std,\n",
    "    phd_productivity + 0.5 * phd_productivity_std,\n",
    "    alpha=0.25, color='C0')\n",
    "plt.scatter(np.arange(len(phd_productivity)), phd_productivity, color='C0')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title(\"Average number of papers, within author's first 5-year period\")\n",
    "plt.ylabel('# papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(phd_start_year) - 5), list(phd_start_year.keys())[:-5], rotation=-45)\n",
    "plt.grid(linestyle='--')\n",
    "# plt.savefig('plots/fig__phd_all.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "phd_first_author = np.array([np.mean([sum([pp[1] > 1 and pp[0] == 1 for pp in aa]) for aa in yy]) for yy in phd_start_year.values()])[:-5]\n",
    "phd_first_author_std = np.array([np.std([sum([pp[1] > 1 and pp[0] == 1 for pp in aa]) for aa in yy]) for yy in phd_start_year.values()])[:-5]\n",
    "plt.plot(phd_first_author)\n",
    "plt.fill_between(\n",
    "    np.arange(len(phd_first_author)),\n",
    "    phd_first_author - 0.5 * phd_first_author_std,\n",
    "    phd_first_author + 0.5 * phd_first_author_std,\n",
    "    alpha=0.25)\n",
    "plt.scatter(np.arange(len(phd_first_author)), phd_first_author, color='C0')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title(\"Average number first-author papers, within author's first 5-year period\")\n",
    "plt.ylabel('# papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(phd_start_year) - 5), list(phd_start_year.keys())[:-5], rotation=-45)\n",
    "plt.grid(linestyle='--')\n",
    "# plt.savefig('plots/fig__phd_first.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phd_last_author = np.array([np.mean([sum([pp[0] == pp[1]  for pp in aa]) for aa in yy]) for yy in phd_start_year.values()])[:-5]\n",
    "phd_last_author_std = np.array([np.std([sum([pp[0] == pp[1]  for pp in aa]) for aa in yy]) for yy in phd_start_year.values()])[:-5]\n",
    "\n",
    "plt.plot(phd_last_author)\n",
    "plt.fill_between(\n",
    "    np.arange(len(phd_last_author)),\n",
    "    phd_last_author - 0.5 * phd_last_author_std,\n",
    "    phd_last_author + 0.5 * phd_last_author_std,\n",
    "    alpha=0.25)\n",
    "plt.scatter(np.arange(len(phd_last_author)), phd_last_author, color='C0')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title(\"Average number last-author papers, within author's first 5-year period\")\n",
    "plt.ylabel('# papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(phd_start_year) - 5), list(phd_start_year.keys())[:-5], rotation=-45)\n",
    "plt.grid(linestyle='--')\n",
    "# plt.savefig('plots/fig__phd_last.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Institution statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More interesting insights come from the authors' institution affiliations, and their participation over the years. Since the first conference there was a total of 2672 participating institutions.\n",
    "\n",
    "Disclaimer:\n",
    "- Affiliation information was not available before 2013\n",
    "- The actual institution names are sometimes inconsistent (e.g. \"Harvard Unviersity\" (small typo), \"Harvard University\", \"Harvard College\", \"Harvard\", \"Harvard Medical School\", \"Harvard/MIT\", etc) and this might lead to some inaccuracies in the final results. However, even though that these examples are counted as separate institutions, we can still get good estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "[dd for dd in total_institutions.keys() if 'Harvard' in dd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_institutions = [len(ss) for ss in year_stats['unique_institutions_info']]\n",
    "plt.bar(np.arange(len(unique_institutions)), unique_institutions)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Total unique institutions at conference')\n",
    "plt.ylabel('# institutions')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "# plt.savefig('plots/fig__unique_institutions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "papers_per_institution = [np.mean([vv for vv in ss.values()]) for ss in year_stats['unique_institutions_info']]\n",
    "papers_per_institution_std = [np.std([vv for vv in ss.values()]) for ss in year_stats['unique_institutions_info']]\n",
    "plt.bar(np.arange(len(papers_per_institution)), papers_per_institution, yerr=papers_per_institution_std)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average number of papers per institution')\n",
    "plt.ylabel('# papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "# plt.savefig('plots/fig__papers_per_institution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 50 publishing institutions with the highest number of papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_papers = [list(vv.values())[0] for vv in total_institutions_sorted][-50:]\n",
    "n_institutions = [list(vv.keys())[0] for vv in total_institutions_sorted][-50:]\n",
    "\n",
    "plt.bar(np.arange(len(n_papers)), n_papers)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Cumulative total papers by institution')\n",
    "plt.ylabel('# of papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(n_institutions)), n_institutions, rotation=-90)\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "# plt.savefig('plots/fig__all_institutions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly as before, the estimated Gini coefficient below shows the inequality of the distribution of papers per institution. In this case, the coefficient of 0.68 indicates a much higher inequality than with individual authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = [list(vv.values())[0] for vv in total_institutions_sorted]\n",
    "share_real = np.cumsum(gg)\n",
    "share_ideal = np.cumsum(np.ones_like(share_real) * share_real[-1] / len(share_real))\n",
    "share_diff = np.sum(share_ideal - share_real)\n",
    "gini_coef = share_diff / np.sum(share_ideal)\n",
    "plt.fill_between(np.arange(len(total_institutions_sorted)), 0, share_real)\n",
    "plt.fill_between(np.arange(len(total_institutions_sorted)), share_real, share_ideal)\n",
    "plt.title(\"Gini coefficient: {}\".format(gini_coef.round(2)))\n",
    "plt.gca().set_aspect(1/(share_ideal[-1]/len(share_ideal)), adjustable='box')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 8)\n",
    "# plt.savefig('plots/fig__all_institutions__gini.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "# http://kimberlyfessel.com/mathematics/applications/gini-use-cases/\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A proxy for an indicator of collaboration, can be obtained via the average number of authors per paper. This number has been steadily increasing, which could indicate more complex research projects published, more openness to collaboration, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_per_paper_mean = np.array([np.mean([len(pp['authors']) for pp in conf_data[kk]]) for kk in sorted(conf_data.keys())])\n",
    "authors_per_paper_std = np.array([np.std([len(pp['authors']) for pp in conf_data[kk]]) for kk in sorted(conf_data.keys())])\n",
    "plt.plot(authors_per_paper_mean, color='C0')\n",
    "plt.fill_between(\n",
    "    np.arange(len(conf_data)),\n",
    "    authors_per_paper_mean - 0.5 * authors_per_paper_std,\n",
    "    authors_per_paper_mean + 0.5 * authors_per_paper_std,\n",
    "    alpha=0.25, color='C0')\n",
    "plt.scatter(np.arange(len(authors_per_paper_mean)), authors_per_paper_mean, color='C0')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average number of authors per paper')\n",
    "plt.ylabel('# authors')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--')\n",
    "# plt.savefig('plots/fig__authors_per_paper.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding only  single author papers, this trend has significantly gone down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "single_authors = 100 * np.array([np.mean([len(pp['authors'])==1 for pp in conf_data[kk]]) for kk in sorted(conf_data.keys())])\n",
    "plt.bar(np.arange(len(single_authors)), single_authors)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Percentage of single author papers')\n",
    "plt.ylabel('% papers')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "# plt.savefig('plots/fig__single_author.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the number of institutions per paper has not significantly changed:(variance is shown as vertical black lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "institutions_per_paper = [np.mean([len(pp['institutions']) for pp in conf_data[kk] if len(pp['institutions']) and pp['institutions'][0] is not None and pp['institutions'][0] != '']) for kk in sorted(conf_data.keys())]\n",
    "institutions_per_paper_std = [np.std([len(pp['institutions']) for pp in conf_data[kk] if len(pp['institutions']) and pp['institutions'][0] is not None and pp['institutions'][0] != '']) for kk in sorted(conf_data.keys())]\n",
    "\n",
    "plt.bar(np.arange(len(institutions_per_paper)), institutions_per_paper, yerr=institutions_per_paper_std)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average number of institutions per paper')\n",
    "plt.ylabel('# authors')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "# plt.savefig('plots/fig__institutions_per_paper.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funding statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting observation is the relationship between the GDP of the institution's country w.r.t. the number of accepted papers. The country information is not included in the paper metadata, so it needs to be inferred based on the institution name, by scraping wikipedia. Regarding GDP info, I downloaded the spreadsheet file from https://databank.worldbank.org/data/download/GDP.xls and parsed the data for the exctracted countries for 2019.\n",
    "\n",
    "It would be interesting to understand how the GDP of a country relates to the available research funding, and subsequently if there is a correlation with the number of accepted papers.\n",
    "\n",
    "We can see a correlation in both standard and log-plots, with the US being a clear front runner. If we assume that the wealth is connected with the available research funding, which could explain such correlation ...\n",
    "\n",
    "we can see a slight positive correlation between the country's GDP and the paper output its institutions produced. To quantify the correlation, I calculated the Pearson's correlation coefficient $\\rho = 0.83$ with the significance $p=4.75\\times10^{-10}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('data/country_papers.pkl', 'rb') as handle:\n",
    "    country_papers = pickle.load(handle)\n",
    "with open('data/country_gdp.pkl', 'rb') as handle:\n",
    "    country_gdp = pickle.load(handle)\n",
    "    \n",
    "data_gdp = []\n",
    "data_paper = []\n",
    "\n",
    "# Plot datapoints\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_size_inches(20, 10)\n",
    "NUM_COLORS = len(country_papers)\n",
    "cm = plt.get_cmap('tab20')\n",
    "ax[0].set_prop_cycle('color', [cm(i % 20) for i in range(NUM_COLORS)])\n",
    "ax[1].set_prop_cycle('color', [cm(i % 20) for i in range(NUM_COLORS)])\n",
    "for i, c_names in enumerate(country_papers.keys()):\n",
    "    if c_names in country_gdp.keys():\n",
    "        data_gdp.append(country_gdp[c_names])\n",
    "        data_paper.append(country_papers[c_names])\n",
    "        ax[0].scatter(country_gdp[c_names], country_papers[c_names], s=50, label=c_names, marker='o' if i<20 else 'p')\n",
    "        ax[1].scatter(country_gdp[c_names], country_papers[c_names], s=50, label=c_names, marker='o' if i<20 else 'p')\n",
    "\n",
    "# Calculate the pearson's correlation coefficient\n",
    "corr, pval = stats.pearsonr(data_gdp, data_paper)\n",
    "logcorr, logpval = stats.pearsonr(np.log(data_gdp), np.log(data_paper))\n",
    "\n",
    "# Fit a line\n",
    "x = np.linspace(min(data_gdp), max(data_gdp), 100)\n",
    "res = stats.linregress(data_gdp, data_paper)\n",
    "ax[0].plot(x, res.intercept + res.slope * x, 'k--', color=\"0.5\")\n",
    "\n",
    "logres = stats.linregress(np.log(data_gdp), np.log(data_paper))\n",
    "ax[1].plot(x, np.exp(logres.intercept + logres.slope * np.log(x)), 'k--', color=\"0.5\")\n",
    "\n",
    "# Plot\n",
    "fig.suptitle('Country GDP vs total accepted papers')\n",
    "ax[0].set_title(r'Normal plot (slope = {:.2e}; $\\rho = {}$)'.format(res.slope, corr.round(2)))\n",
    "ax[0].set_xlabel('GDP [$]')\n",
    "ax[0].set_ylabel('# papers')\n",
    "ax[1].set_title(r'Log plot (slope = {}; $\\rho = {}$)'.format(logres.slope.round(2), logcorr.round(2)))\n",
    "ax[1].set_xlabel('log GDP [$]')\n",
    "ax[1].set_ylabel('log # papers')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_yscale('log')\n",
    "lgd = ax[1].legend(ncol=1, loc='upper left', bbox_to_anchor=(1, 0., 1, 1))\n",
    "# plt.savefig('plots/fig__country_vs_gdp.png', dpi=300, bbox_inches='tight', bbox_extra_artists=(lgd,))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Average number of citations per paper (although this is biased as older papers have been around more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_citations = [np.mean([vv['citations'] for vv in vals if vv['citations'] is not None]) for vals in conf_data.values()]\n",
    "n_citations_std = [np.std([vv['citations'] for vv in vals if vv['citations'] is not None]) for vals in conf_data.values()]\n",
    "plt.bar(np.arange(len(n_citations)), n_citations)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average number of citations per paper')\n",
    "plt.ylabel('# reviews')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "# plt.savefig('plots/fig__citations_per_paper.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of reviews per paper\n",
    "- Average reviewer confidence\n",
    "- Average review length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reviews = [np.mean([len(vv['reviews']) for vv in vals if vv['reviews'] is not None]) for vals in conf_data.values()]\n",
    "n_reviews_std = [np.std([len(vv['reviews']) for vv in vals if vv['reviews'] is not None]) for vals in conf_data.values()]\n",
    "plt.bar(np.arange(len(conf_data)), n_reviews, yerr=n_reviews_std)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average number of reviewers per paper')\n",
    "plt.ylabel('# reviews')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "# plt.savefig('plots/fig__reviews_per_paper.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_score = [np.mean([np.mean([rc['confidence'] for rc in vv['reviews'].values() if rc['confidence'] is not None]) for vv in vals if vv['reviews'] is not None]) for vals in conf_data.values()]\n",
    "review_score_std = [np.std([np.mean([rc['confidence'] for rc in vv['reviews'].values() if rc['confidence'] is not None]) for vv in vals if vv['reviews'] is not None]) for vals in conf_data.values()]\n",
    "plt.bar(np.arange(len(conf_data)), review_score, yerr=review_score_std)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average reviewer confidence')\n",
    "plt.ylabel('confidence score')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "# plt.savefig('plots/fig__reviews_confidence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_words = [np.mean([np.mean([len(rc['text'].split(' ')) for rc in vv['reviews'].values()]) for vv in vals if vv['reviews'] is not None]) for vals in conf_data.values()]\n",
    "review_words_std = [np.std([np.mean([len(rc['text'].split(' ')) for rc in vv['reviews'].values()]) for vv in vals if vv['reviews'] is not None]) for vals in conf_data.values()]\n",
    "plt.bar(np.arange(len(conf_data)), review_words, yerr=review_words_std)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.title('Average review length')\n",
    "plt.ylabel('# words')\n",
    "plt.ylim(0)\n",
    "plt.xticks(np.arange(len(conf_data)), sorted(conf_data.keys()), rotation=-45)\n",
    "plt.grid(linestyle='--', axis='y')\n",
    "# plt.savefig('plots/fig__reviews_length.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
