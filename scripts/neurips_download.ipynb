{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nips_papers = 'https://papers.nips.cc/'\n",
    "html_text = requests.get(nips_papers).text\n",
    "soup = BeautifulSoup(html_text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_citations(title, authors, year):\n",
    "    ### Has issues, check\n",
    "    ### Has issues, check\n",
    "    authors = '+'.join(['+'.join([aa['given_name'], aa['family_name']]) for aa in authors])\n",
    "    req_string = 'https://scholar.google.co.uk/scholar?as_q=&as_epq={title}&as_occt=title&as_sauthors={authors}&as_publication='.format(\n",
    "        title=title, authors=authors)\n",
    "    html_text = requests.get(req_string).text\n",
    "    re_result = re.search('>Cited by (.*?)</a>', html_text)\n",
    "    num_citations = None if re_result is None else int(re_result.group(1))\n",
    "    return num_citations\n",
    "\n",
    "def get_reviews(html_handle):\n",
    "    reviewer_soup = BeautifulSoup(html_handle, 'html.parser')\n",
    "    reviewer_dict = {}\n",
    "    for reviewer in reviewer_soup.find_all('h3'):\n",
    "        review_text = ''\n",
    "        for sib in reviewer.find_next_siblings():\n",
    "            if sib.name == \"h3\":\n",
    "                break\n",
    "            else:\n",
    "                review_text += ' ' + sib.text\n",
    "        re_result = re.search('Confidence in this Review (.*?)-', review_text)\n",
    "        review_conf = None if re_result is None else int(re_result.group(1))\n",
    "        reviewer_dict[reviewer.contents[0]] = {\n",
    "            'text': review_text, 'confidence': review_conf}\n",
    "    return reviewer_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if there is already some data\n",
    "if os.path.isfile('neurips_conf_data.pkl'):\n",
    "    with open('neurips_conf_data.pkl', 'rb') as handle:\n",
    "        conf_data = pickle.load(handle)\n",
    "else:\n",
    "    conf_data = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Processing:  Advances in Neural Information Processing Systems 33  pre-proceedings (NeurIPS 2020)\n",
      " - paper [1/1898]: A graph similarity for deep learning\n",
      " - paper [2/1898]: An Unsupervised Information-Theoretic Perceptual Quality Metric\n",
      " - paper [3/1898]: Self-Supervised MultiModal Versatile Networks\n",
      " - paper [4/1898]: Benchmarking Deep Inverse Models over time, and the Neural-Adjoint method\n",
      " - paper [5/1898]: Off-Policy Evaluation and Learning for External Validity under a Covariate Shift\n",
      " - paper [6/1898]: Neural Methods for Point-wise Dependency Estimation\n",
      " - paper [7/1898]: Fast and Flexible Temporal Point Processes with Triangular Maps\n",
      " - paper [8/1898]: Backpropagating Linearly Improves Transferability of Adversarial Examples\n",
      " - paper [9/1898]: PyGlove: Symbolic Programming for Automated Machine Learning\n",
      " - paper [10/1898]: Fourier Sparse Leverage Scores and Approximate Kernel Learning\n",
      " - paper [11/1898]: Improved Algorithms for Online Submodular Maximization via First-order Regret Bounds\n",
      " - paper [12/1898]: Synbols: Probing Learning Algorithms with Synthetic Datasets\n",
      " - paper [13/1898]: Adversarially Robust Streaming Algorithms via Differential Privacy\n",
      " - paper [14/1898]: Trading Personalization for Accuracy: Data Debugging in Collaborative Filtering\n",
      " - paper [15/1898]: Cascaded Text Generation with Markov Transformers\n",
      " - paper [16/1898]: Improving Local Identifiability in Probabilistic Box Embeddings\n",
      " - paper [17/1898]: Permute-and-Flip: A new mechanism for differentially private selection\n",
      " - paper [18/1898]: Deep reconstruction of strange attractors from time series\n",
      " - paper [19/1898]: Reciprocal Adversarial Learning via Characteristic Functions\n",
      " - paper [20/1898]: Statistical Guarantees of Distributed Nearest Neighbor Classification\n",
      " - paper [21/1898]: Stein Self-Repulsive Dynamics: Benefits From Past Samples\n",
      " - paper [22/1898]: The Statistical Complexity of Early-Stopped Mirror Descent\n",
      " - paper [23/1898]: Algorithmic recourse under imperfect causal knowledge: a probabilistic approach\n",
      " - paper [24/1898]: Quantitative Propagation of Chaos for SGD in Wide Neural Networks\n",
      " - paper [25/1898]: A Causal View on Robustness  of Neural Networks\n",
      " - paper [26/1898]: Minimax Classification with 0-1 Loss and Performance Guarantees\n",
      " - paper [27/1898]: How to Learn a Useful Critic? Model-based Action-Gradient-Estimator Policy Optimization\n",
      " - paper [28/1898]: Coresets for Regressions with Panel Data\n",
      " - paper [29/1898]: Learning Composable Energy Surrogates for PDE Order Reduction\n",
      " - paper [30/1898]: Efficient Contextual Bandits with Continuous Actions\n",
      " - paper [31/1898]: Achieving Equalized Odds by Resampling Sensitive Attributes\n",
      " - paper [32/1898]: Multi-Robot Collision Avoidance under Uncertainty with Probabilistic Safety Barrier Certificates\n",
      " - paper [33/1898]: Hard Shape-Constrained Kernel Machines\n",
      " - paper [34/1898]: A Closer Look at the Training Strategy for Modern Meta-Learning\n",
      " - paper [35/1898]: On the Value of Out-of-Distribution Testing: An Example of Goodhart's Law\n",
      " - paper [36/1898]: Generalised Bayesian Filtering via Sequential Monte Carlo\n",
      " - paper [37/1898]: Deterministic Approximation for Submodular Maximization over a Matroid in Nearly Linear Time\n",
      " - paper [38/1898]: Flows for simultaneous manifold learning and density estimation\n",
      " - paper [39/1898]: Simultaneous Preference and Metric Learning from Paired Comparisons\n",
      " - paper [40/1898]: Efficient Variational Inference for Sparse Deep Learning with Theoretical Guarantee\n",
      " - paper [41/1898]: Learning Manifold Implicitly via Explicit Heat-Kernel Learning\n",
      " - paper [42/1898]: Deep Relational Topic Modeling via Graph Poisson Gamma Belief Network\n",
      " - paper [43/1898]: One-bit Supervision for Image Classification\n",
      " - paper [44/1898]: What is being transferred in transfer learning? \n",
      " - paper [45/1898]: Submodular Maximization Through Barrier Functions\n",
      " - paper [46/1898]: Neural Networks with Recurrent Generative Feedback\n",
      " - paper [47/1898]: Learning to Extrapolate Knowledge: Transductive Few-shot Out-of-Graph Link Prediction\n",
      " - paper [48/1898]: Exploiting weakly supervised visual patterns to learn from partial annotations\n",
      " - paper [49/1898]: Improving Inference for Neural Image Compression\n",
      " - paper [50/1898]: Neuron Merging: Compensating for Pruned Neurons\n",
      " - paper [51/1898]: FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence\n",
      " - paper [52/1898]: Reinforcement Learning with Combinatorial Actions: An Application to Vehicle Routing\n",
      " - paper [53/1898]: Towards Playing Full MOBA Games with Deep Reinforcement Learning\n",
      " - paper [54/1898]: Rankmax: An Adaptive Projection Alternative to the Softmax Function\n",
      " - paper [55/1898]: Online Agnostic Boosting via Regret Minimization\n",
      " - paper [56/1898]: Causal Intervention for Weakly-Supervised Semantic Segmentation\n",
      " - paper [57/1898]: Belief Propagation Neural Networks\n",
      " - paper [58/1898]: Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality\n",
      " - paper [59/1898]: Post-training Iterative Hierarchical Data Augmentation for Deep Networks\n",
      " - paper [60/1898]: Debugging Tests for Model Explanations\n",
      " - paper [61/1898]:  Robust compressed sensing using generative models \n",
      " - paper [62/1898]: Fairness without Demographics through Adversarially Reweighted Learning\n",
      " - paper [63/1898]: Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model\n",
      " - paper [64/1898]: Ridge Rider: Finding Diverse Solutions by Following Eigenvectors of the Hessian\n",
      " - paper [65/1898]: The route to chaos in routing games: When is price of anarchy too optimistic?\n",
      " - paper [66/1898]: Online Algorithm for Unsupervised Sequential Selection with Contextual Information\n",
      " - paper [67/1898]: Adapting Neural Architectures Between Domains\n",
      " - paper [68/1898]: What went wrong and when? Instance-wise feature importance for time-series black-box models\n",
      " - paper [69/1898]: Towards Better Generalization of Adaptive Gradient Methods\n",
      " - paper [70/1898]: Learning Guidance Rewards with Trajectory-space Smoothing\n",
      " - paper [71/1898]: Variance Reduction via Accelerated Dual Averaging for Finite-Sum Optimization\n",
      " - paper [72/1898]: Tree! I am no Tree! I am a low dimensional Hyperbolic Embedding\n",
      " - paper [73/1898]: Deep Structural Causal Models for Tractable Counterfactual Inference\n",
      " - paper [74/1898]: Convolutional Generation of Textured 3D Meshes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loop through all conference years\n",
    "all_conferences = [cc for cc in soup.find_all('li') if 'paper' in cc.a.get('href')]\n",
    "all_conferences = all_conferences[::-1]\n",
    "for cc in all_conferences[len(conf_data):]:\n",
    "    conf_link = urljoin(nips_papers, cc.a.get('href'))\n",
    "    conf_year = conf_link.split('/')[-1]  \n",
    "    html_text = requests.get(conf_link).text\n",
    "    conf = BeautifulSoup(html_text, 'html.parser')\n",
    "    \n",
    "    # Loop through all current conference's papers\n",
    "    print(\"\\n\\nProcessing: \", cc.a.contents[0])\n",
    "    paper_list = []\n",
    "    all_papers = [pp for pp in conf.find_all('li') if 'paper' in pp.a.get('href')]\n",
    "    for pi, pp in enumerate(all_papers):\n",
    "        # Get paper info\n",
    "        print(\" - paper [{}/{}]: {}\".format(pi + 1, len(all_papers), pp.a.contents[0]))\n",
    "        paper_link = urljoin(conf_link, pp.a.get('href')) \n",
    "        link_file = paper_link.replace('hash', 'file')\n",
    "        # Extract paper metadata\n",
    "        link_meta = link_file.replace('html', 'json')\n",
    "        link_meta = link_meta.replace('Abstract', 'Metadata')\n",
    "        html_text = requests.get(link_meta).text\n",
    "        if html_text == 'Resource Not Found':\n",
    "            html_ = requests.get(paper_link).text\n",
    "            conf = BeautifulSoup(html_, 'html.parser')\n",
    "            abstract_text = conf.find('h4', string='Abstract').next_sibling.next_sibling.contents[0]\n",
    "            abstract = None if abstract_text == 'Abstract Unavailable' else abstract_text\n",
    "            abstract = abstract.replace('<p>', '')\n",
    "            abstract = abstract.replace('</p>', '')\n",
    "            abstract = abstract.replace('\\n', ' ')\n",
    "            author_list = [\n",
    "                {'given_name': aa.split(' ')[0],\n",
    "                 'family_name': aa.split(' ')[1],\n",
    "                 'institution': None} for aa in pp.i.contents[0].split(', ')]\n",
    "            paper_meta = {\n",
    "                'title': pp.a.contents[0],\n",
    "                'authors': author_list,\n",
    "                'abstract': abstract,\n",
    "                'full_text': None\n",
    "            }\n",
    "        else:\n",
    "            paper_meta = json.loads(html_text)\n",
    "            if 'full_text' in paper_meta.keys():\n",
    "                paper_meta['full_text'] = paper_meta['full_text'].replace('\\n', ' ')\n",
    "        # Extract paper supplemental\n",
    "        link_supplement = link_file.replace('html', 'zip')\n",
    "        link_supplement = link_supplement.replace('Abstract', 'Supplemental')\n",
    "        html_text = requests.get(link_supplement).text\n",
    "        if html_text == 'Resource Not Found':\n",
    "            has_zip = False\n",
    "        else:\n",
    "            has_zip = True\n",
    "        link_supplement = link_supplement.replace('zip', 'pdf')\n",
    "        html_text = requests.get(link_supplement).text\n",
    "        if html_text == 'Resource Not Found':\n",
    "            has_pdf = False\n",
    "        else:\n",
    "            has_pdf = True\n",
    "        has_supplement = has_pdf or has_zip\n",
    "        # Extract paper reviews\n",
    "        link_review = link_file.replace('Abstract', 'Reviews')\n",
    "        html_text = requests.get(link_review).text\n",
    "        if html_text == 'Resource Not Found':\n",
    "            reviews = None\n",
    "        else:\n",
    "            reviews = get_reviews(html_text)\n",
    "        # Extract scholar citation data\n",
    "        num_cit = get_num_citations(title=paper_meta['title'], authors=paper_meta['authors'], year=conf_year)\n",
    "        # Update paper info\n",
    "        paper_meta.update({\n",
    "            'year': conf_year,\n",
    "            'citations': num_cit,\n",
    "            'institutions': list(set([aa['institution'] for aa in paper_meta['authors']])),\n",
    "            'reviews': reviews,\n",
    "            'has_supplement': has_supplement})\n",
    "        paper_list.append(paper_meta)\n",
    "        \n",
    "    # Update conference info\n",
    "    conf_data[conf_year] = paper_list\n",
    "    with open('neurips_conf_data.pkl', 'wb') as handle:\n",
    "        pickle.dump(conf_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
